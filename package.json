{
  "_args": [
    [
      {
        "raw": "scraper@https://github.com/hashashin/node-scraper/tarball/master",
        "scope": null,
        "escapedName": "scraper",
        "name": "scraper",
        "rawSpec": "https://github.com/hashashin/node-scraper/tarball/master",
        "spec": "https://github.com/hashashin/node-scraper/tarball/master",
        "type": "remote"
      },
      "/tmp/build_eed1b30b9710ed6c7a5271960c1f4713/hashashin-ambrosio-tele-25ec435"
    ]
  ],
  "_from": "https://github.com/hashashin/node-scraper/tarball/master",
  "_id": "scraper@0.0.9",
  "_inCache": true,
  "_location": "/scraper",
  "_phantomChildren": {
    "contextify": "0.1.15",
    "cssom": "0.3.2",
    "cssstyle": "0.2.37",
    "htmlparser2": "3.8.3",
    "nwmatcher": "1.3.9",
    "request": "2.81.0",
    "xmlhttprequest": "1.8.0"
  },
  "_requested": {
    "raw": "scraper@https://github.com/hashashin/node-scraper/tarball/master",
    "scope": null,
    "escapedName": "scraper",
    "name": "scraper",
    "rawSpec": "https://github.com/hashashin/node-scraper/tarball/master",
    "spec": "https://github.com/hashashin/node-scraper/tarball/master",
    "type": "remote"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "https://github.com/hashashin/node-scraper/tarball/master",
  "_shasum": "3117d9786cf2826fe4b75b651cceffc5d4f9d35a",
  "_shrinkwrap": null,
  "_spec": "scraper@https://github.com/hashashin/node-scraper/tarball/master",
  "_where": "/tmp/build_eed1b30b9710ed6c7a5271960c1f4713/hashashin-ambrosio-tele-25ec435",
  "author": {
    "name": "Mathias Pettersson",
    "email": "mape@mape.me"
  },
  "bugs": {
    "url": "https://github.com/hashashin/node-scraper/issues"
  },
  "dependencies": {
    "jsdom": ">=0.1.20",
    "request": ">=0.10.0"
  },
  "description": "Easier web scraping using jQuery.",
  "devDependencies": {},
  "directories": {
    "lib": "./lib"
  },
  "engines": [
    "node"
  ],
  "homepage": "https://github.com/hashashin/node-scraper#readme",
  "main": "./lib/scraper",
  "name": "scraper",
  "optionalDependencies": {},
  "readme": "# node-scraper\n\nA little module that makes scraping websites a little easier. Uses node.js and jQuery.\n\n## Installation\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install scraper\n\n## Examples\n\n### Simple\nFirst argument is an url as a string, second is a callback which exposes a jQuery object with your scraped site as \"body\" and third is an object from the request containing info about the url.\n\n    var scraper = require('scraper');\n    scraper('http://search.twitter.com/search?q=javascript', function(err, jQuery) {\n        if (err) {throw err}\n\n        jQuery('.msg').each(function() {\n            console.log(jQuery(this).text().trim()+'\\n');\n        });\n    });\n### \"Advanced\"\nFirst argument is an object containing settings for the \"request\" instance used internally, second is a callback which exposes a jQuery object with your scraped site as \"body\" and third is an object from the request containing info about the url.\n\n    var scraper = require('scraper');\n    scraper(\n\t    {\n           'uri': 'http://search.twitter.com/search?q=nodejs'\n               , 'headers': {\n                   'User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'\n               }\n        }\n        , function(err, $) {\n            if (err) {throw err}\n\n            $('.msg').each(function() {\n                console.log($(this).text().trim()+'\\n');\n            });\n        }\n    );\n### Parallel\nFirst argument is an array containing either strings or objects, second is a callback which exposes a jQuery object with your scraped site as \"body\" and third is an object from the request containing info about the url.\n\n**You can also add rate limiting to the fetcher by adding an options object as the third argument containing 'reqPerSec': float.**\n\n    var scraper = require('scraper');\n    scraper(\n\t    [\n            'http://search.twitter.com/search?q=javascript'\n            , 'http://search.twitter.com/search?q=css'\n            , {\n                'uri': 'http://search.twitter.com/search?q=nodejs'\n                , 'headers': {\n                    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'\n                }\n            }\n            , 'http://search.twitter.com/search?q=html5'\n        ]\n        , function(err, $) {\n            if (err) {throw err;}\n\n            $('.msg').each(function() {\n                console.log($(this).text().trim()+'\\n');\n            });\n        }\n        , {\n            'reqPerSec': 0.2 // Wait 5sec between each external request\n        }\n    );\n\n\n\n## Arguments\n\n### First (required)\nContains the info about what page/pages will be scraped\n\n#### string\n    'http://www.nodejs.org'\n**or**\n\n#### request object\n    {\n       'uri': 'http://search.twitter.com/search?q=nodejs'\n           , 'headers': {\n               'User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'\n           }\n    }\n**or**\n\n#### Array (if you want to do fetches on multiple URLs)\n    [\n        urlString\n        , urlString\n        , requestObject\n        , urlString\n    ]\n\n### Second (optional)\nThe callback that allows you do use the data retrieved from the fetch.\n\n    function(err, $) {\n        if (err) {throw err;}\n        \n        $('.msg').each(function() {\n            console.log($(this).text().trim()+'\\n');\n        }\n    }\n\n### Third (optional)\nThis argument is an object containing settings for the fetcher overall.\n\n* **reqPerSec**: float; (allows you to throttle your fetches so you don't hammer the server you are scraping)\n\n## Depends on\n* [tmpvar](https://github.com/tmpvar/)'s [jsdom](https://github.com/tmpvar/jsdom)\n* [mikeal](https://github.com/mikeal/)'s [request](https://github.com/mikeal/node-utils/tree/master/request)\n* [jquery](https://github.com/jquery/jquery)",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/hashashin/node-scraper.git"
  },
  "version": "0.0.9"
}
